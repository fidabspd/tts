{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5460ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from jamo import hangul_to_jamo\n",
    "\n",
    "from transformer_torch import *\n",
    "from preprocess import *\n",
    "import configs as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17979686",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../model/'\n",
    "MODEL_NAME = 'single_speaker_tts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55e383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34d4de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torch.load(MODEL_PATH+MODEL_NAME+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a476d2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ['여1_소설1', '여1_자기계발2', '여1_동화1', '여1_자기계발1'] ...\n",
      "소설1 Done!\n",
      "자기계발2 Done!\n",
      "동화1 Done!\n",
      "자기계발1 Done!\n"
     ]
    }
   ],
   "source": [
    "ds = get_single_speaker_dataset(\n",
    "    cf.SPEAKER, cf.WAV_PATH, cf.SCRIPT_FILE_NAME, cf.SR, cf.N_MELS, cf.N_FFT, cf.HOP_LENGTH, cf.WIN_LENGTH)\n",
    "dl = DataLoader(ds, batch_size=cf.BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1de2373",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot apply_along_axis when any iteration dimensions are 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_229/1758156261.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_padding_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mt_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tts/codes/transformer_torch.py\u001b[0m in \u001b[0;36mcreate_padding_mask\u001b[0;34m(self, key, for_speech)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mspeech_without_sos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mspeech_not_pad_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mspeech_without_sos\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mspeech_not_pad_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mor_for_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeech_not_pad_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             speech_with_sos = np.concatenate([\n\u001b[1;32m    277\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m         raise ValueError(\n\u001b[1;32m    377\u001b[0m             \u001b[0;34m'Cannot apply_along_axis when any iteration dimensions are 0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         ) from None\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot apply_along_axis when any iteration dimensions are 0"
     ]
    }
   ],
   "source": [
    "# 2번째 axis의 len이 1이면 np.apply_along_axis가 작동하지 않는다.\n",
    "t = torch.rand((2, 1, 80))\n",
    "\n",
    "t_mask = transformer.create_padding_mask(t, True)\n",
    "t_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ed8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot():\n",
    "\n",
    "    def __init__(self, transformer, device):\n",
    "        self.transformer = transformer.to(device)\n",
    "        self.transformer.eval()\n",
    "        self.device = device\n",
    "\n",
    "    def qna(self, question):\n",
    "\n",
    "        speech_seq_len = self.transformer.speech_seq_len\n",
    "        \n",
    "        question = '@'+question+'|'\n",
    "        question = normalize_text(question)\n",
    "        question = list(hangul_to_jamo(question))\n",
    "        question_tokens = [cf.char_to_id[_] for _ in question]\n",
    "        question_tokens = torch.LongTensor(question_tokens).unsqueeze(0).to(self.device)\n",
    "        question_mask = self.transformer.create_padding_mask(question_tokens)\n",
    "        with torch.no_grad():\n",
    "            question_encd = self.transformer.encoder(question_tokens, question_mask)\n",
    "\n",
    "        output_tokens = torch.zeros([1, cf.N_MELS]).unsqueeze(0).to(self.device)\n",
    "\n",
    "        for i in range(speech_seq_len):\n",
    "            target_tokens = output_tokens\n",
    "\n",
    "            ##################################################################\n",
    "            if len(target_tokens) == 1:\n",
    "                target_mask = self.transformer.create_padding_mask(target_tokens.tile(2,1,1), True)[:1]\n",
    "            else:\n",
    "                target_mask = self.transformer.create_padding_mask(target_tokens, True)\n",
    "#             target_mask = self.transformer.create_padding_mask(target_tokens, True)\n",
    "            ##################################################################\n",
    "            with torch.no_grad():\n",
    "                output, _, attention = self.transformer.decoder(target_tokens, question_encd, target_mask, question_mask)\n",
    "\n",
    "            output_tokens = torch.concat((\n",
    "                    torch.zeros([1, cf.N_MELS]).unsqueeze(0).to(self.device),\n",
    "                    output[:, :i+1, :]\n",
    "            ), axis=1)\n",
    "                \n",
    "        answer = output_tokens\n",
    "        \n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.attention = attention\n",
    "        self.call_qna = True\n",
    "        \n",
    "        return answer, attention\n",
    "\n",
    "#     def plot_attention_weights(self, draw_mean=False):\n",
    "#         if not self.call_qna:\n",
    "#             raise Exception('There is no `question`, `answer` and `attention`. Call `qna` first')\n",
    "#         question_token = to_tokens(self.question, self.tokenizer, to_ids=False)\n",
    "#         question_token = ['<sos>']+question_token+['<eos>']\n",
    "\n",
    "#         answer_token = to_tokens(self.answer, self.tokenizer, to_ids=False)\n",
    "#         answer_token = answer_token+['<eos>']\n",
    "\n",
    "#         attention = self.attention.squeeze(0)\n",
    "#         if draw_mean:\n",
    "#             attention = torch.mean(attention, dim=0, keepdim=True)\n",
    "#         attention = attention.cpu().detach().numpy()\n",
    "\n",
    "#         n_col = 4\n",
    "#         n_row = (attention.shape[0]-1)//n_col + 1\n",
    "#         fig = plt.figure(figsize = (n_col*6, n_row*6))\n",
    "#         for i in range(attention.shape[0]):\n",
    "#             plt.subplot(n_row, n_col, i+1)\n",
    "#             plt.matshow(attention[i], fignum=False)\n",
    "#             plt.xticks(range(len(question_token)), question_token, rotation=45)\n",
    "#             plt.yticks(range(len(answer_token)), answer_token)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e83157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = Chatbot(transformer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57056b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b73f68f4",
   "metadata": {},
   "source": [
    "# mel to audio sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8adab9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_SAVE_PATH = '../audio_sample/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b573968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "fpath = '../data/wav/여1_동화1/1.wav'\n",
    "\n",
    "origin, _ = librosa.load(fpath, sr=cf.SR)\n",
    "mel = get_mel(fpath, cf.SR, cf.N_MELS, cf.N_FFT, cf.HOP_LENGTH, cf.WIN_LENGTH)\n",
    "\n",
    "inversed = librosa.feature.inverse.mel_to_audio(mel.T, sr=cf.SR, hop_length=cf.HOP_LENGTH, win_length=cf.WIN_LENGTH)\n",
    "\n",
    "sf.write(AUDIO_SAVE_PATH+'test_origin.wav', origin, cf.SR)\n",
    "sf.write(AUDIO_SAVE_PATH+'test_inversed.wav', inversed, cf.SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eba7c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = iter(dl).next()\n",
    "\n",
    "sample_text, sample_speech = sample[0].to(device), sample[1].to(device)\n",
    "\n",
    "outputs, _, _ = transformer(sample_text, sample_speech)\n",
    "\n",
    "mel_pred = outputs[0].detach().cpu().numpy()\n",
    "\n",
    "pred_speech = librosa.feature.inverse.mel_to_audio(mel_pred.T, sr=cf.SR, hop_length=cf.HOP_LENGTH, win_length=cf.WIN_LENGTH)\n",
    "\n",
    "origin_text = ''.join([cf.id_to_char[_] for _ in sample_text[0].detach().cpu().numpy()])\n",
    "\n",
    "sf.write(AUDIO_SAVE_PATH+'model_output_test.wav', pred_speech, cf.SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b32d276e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"이야기하다 보면 '왜 저 두 사람이 친구인지'를 이해하고 고개를 끄덕이게 되곤 한다.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_text.split('|')[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6011bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
